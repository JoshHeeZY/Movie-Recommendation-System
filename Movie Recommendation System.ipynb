{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3d636dd-8334-43a2-8f66-2f5657c5e5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\joshu\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\joshu\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\joshu\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Use this command in a Jupyter Notebook cell to install required libraries\n",
    "!pip install pandas scikit-learn numpy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "279d94c0-fd84-430c-9767-0b58978c5b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge scikit-surprise --yes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "456dd3a3-e205-440e-9682-5ed239685b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Name', 'Aired Date', 'Year of release', 'Original Network', 'Aired On',\n",
      "       'Number of Episodes', 'Duration', 'Content Rating', 'Rating',\n",
      "       'Synopsis', 'Genre', 'Tags', 'Director', 'Screenwriter', 'Cast',\n",
      "       'Production companies', 'Rank'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "kdramas_df = pd.read_csv('kdrama.csv')\n",
    "\n",
    "# Print the column names\n",
    "print(kdramas_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49d59a91-d7d2-46d6-8793-c646e271009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "kdramas_df = pd.read_csv('kdrama.csv')\n",
    "\n",
    "# Fill missing Synopses with empty strings\n",
    "kdramas_df['Synopsis'] = kdramas_df['Synopsis'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9033852-c003-4a34-8513-60bec26f23c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the Synopses to a TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(kdramas_df['Synopsis'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee96957b-cc1d-4c0e-8cb0-57f27e71c1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64                 Dr. Romantic\n",
      "128                       Blind\n",
      "228                 Light on Me\n",
      "134    The World of the Married\n",
      "95                      Save Me\n",
      "101                Nobody Knows\n",
      "22                       Healer\n",
      "52                    Defendant\n",
      "195                 Oh My Ghost\n",
      "161        My Unfamiliar Family\n",
      "Name: Name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Create a reverse map of indices and K-Drama names\n",
    "indices = pd.Series(kdramas_df.index, index=kdramas_df['Name']).drop_duplicates()\n",
    "\n",
    "def recommend_kdramas(name, cosine_sim=cosine_sim):\n",
    "    # Get the index of the K-Drama that matches the name\n",
    "    if name not in indices:\n",
    "        return f\"No recommendations found for {name}. This K-Drama might not be in the dataset.\"\n",
    "    idx = indices[name]\n",
    "\n",
    "    # Get the pairwise similarity scores of all K-Dramas with that K-Drama\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the K-Dramas based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar K-Dramas\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the K-Drama indices\n",
    "    kdrama_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar K-Dramas\n",
    "    return kdramas_df['Name'].iloc[kdrama_indices]\n",
    "\n",
    "# Example usage - Replace 'Your K-Drama Name Here' with a name from your dataset\n",
    "print(recommend_kdramas('Reply 1988'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "700ae859-2bfb-4736-aef9-7d6b6b37a7c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize TF-IDF Vectorizer and compute the TF-IDF matrix\u001b[39;00m\n\u001b[0;32m     15\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(kdramas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_features\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Compute the cosine similarity matrix based on the 'combined_features'\u001b[39;00m\n\u001b[0;32m     19\u001b[0m cosine_sim \u001b[38;5;241m=\u001b[39m linear_kernel(tfidf_matrix, tfidf_matrix)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2128\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2129\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2130\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2131\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2132\u001b[0m )\n\u001b[1;32m-> 2133\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2135\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1274\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1276\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:106\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m     doc \u001b[38;5;241m=\u001b[39m decoder(doc)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:239\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    236\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    241\u001b[0m     )\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Load the dataset\n",
    "kdramas_df = pd.read_csv('kdrama.csv')\n",
    "\n",
    "# Create a combined string of director, screenwriter, and cast\n",
    "kdramas_df['staff'] = kdramas_df['Director'] + ' ' + kdramas_df['Screenwriter'] + ' ' + kdramas_df['Cast']\n",
    "\n",
    "# Creating the 'combined_features' for content-based filtering\n",
    "kdramas_df['combined_features'] = kdramas_df['Synopsis'] + ' ' + kdramas_df['staff'] + ' ' + kdramas_df['Genre'] + ' ' + kdramas_df['Tags']\n",
    "\n",
    "# Initialize TF-IDF Vectorizer and compute the TF-IDF matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(kdramas_df['combined_features'])\n",
    "\n",
    "# Compute the cosine similarity matrix based on the 'combined_features'\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Reverse mapping of indices and K-Drama names\n",
    "indices = pd.Series(kdramas_df.index, index=kdramas_df['Name']).drop_duplicates()\n",
    "\n",
    "# Recommendation function\n",
    "def recommend_kdramas(name, user_content_rating=None, year=None, top_n=10):\n",
    "    # Get the index of the K-Drama that matches the name\n",
    "    if name not in indices:\n",
    "        return f\"No recommendations found for {name}. This K-Drama might not be in the dataset.\"\n",
    "    \n",
    "    idx = indices[name]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:top_n+1]  # Get top_n recommendations\n",
    "    \n",
    "    # Get the K-Drama indices\n",
    "    kdrama_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    # Start with the top similar K-Dramas\n",
    "    recommended_kdramas = kdramas_df.loc[kdrama_indices].copy()\n",
    "    \n",
    "    # Filter based on 'Content Rating' if specified\n",
    "    if user_content_rating:\n",
    "        recommended_kdramas = recommended_kdramas[recommended_kdramas['Content Rating'] <= user_content_rating]\n",
    "    \n",
    "    # Filter based on 'Year of release' if specified\n",
    "    if year:\n",
    "        recommended_kdramas = recommended_kdramas[recommended_kdramas['Year of release'] == year]\n",
    "    \n",
    "    # Return top N recommendations sorted by 'Rating' and 'Rank'\n",
    "    return recommended_kdramas.sort_values(by=['Rating', 'Rank'], ascending=[False, True])['Name'].head(top_n)\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'Your K-Drama Name Here' with an actual name from your dataset.\n",
    "# 'user_content_rating' could be an age or maturity level (e.g., '18+', '15+', etc.)\n",
    "# 'year' could be the release year of interest for the user.\n",
    "# 'top_n' is the number of recommendations to return.\n",
    "\n",
    "print(recommend_kdramas('Reply 1988', user_content_rating='15+', year=2015, top_n=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8610700-6ef0-47fc-a0e5-5518c494f51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Name of Drama:  Reply 1988\n",
      "Content Rating (e.g., '15+', '18+ Restricted'):  \n",
      "Year of Release (leave blank if not specific):  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2             Hospital Playlist\n",
       "43              Dear My Friends\n",
       "85                   Once Again\n",
       "103             Beautiful World\n",
       "110                  Reply 1997\n",
       "114            Fight For My Way\n",
       "134    The World of the Married\n",
       "161        My Unfamiliar Family\n",
       "175          Seasons of Blossom\n",
       "228                 Light on Me\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Load the dataset\n",
    "kdramas_df = pd.read_csv('kdrama.csv')\n",
    "\n",
    "# Ensure all relevant columns are string type and fill NaN values\n",
    "columns_to_clean = ['Synopsis', 'Director', 'Screenwriter', 'Cast', 'Genre', 'Tags']\n",
    "for column in columns_to_clean:\n",
    "    kdramas_df[column] = kdramas_df[column].fillna('').astype(str)\n",
    "\n",
    "# Create a combined string of director, screenwriter, and cast\n",
    "kdramas_df['staff'] = kdramas_df['Director'] + ' ' + kdramas_df['Screenwriter'] + ' ' + kdramas_df['Cast']\n",
    "\n",
    "# Creating the 'combined_features' for content-based filtering\n",
    "kdramas_df['combined_features'] = kdramas_df['Synopsis'] + ' ' + kdramas_df['staff'] + ' ' + kdramas_df['Genre'] + ' ' + kdramas_df['Tags']\n",
    "\n",
    "# Initialize TF-IDF Vectorizer and compute the TF-IDF matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(kdramas_df['combined_features'])\n",
    "\n",
    "# Compute the cosine similarity matrix based on the 'combined_features'\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Reverse mapping of indices and K-Drama names\n",
    "indices = pd.Series(kdramas_df.index, index=kdramas_df['Name']).drop_duplicates()\n",
    "\n",
    "# Interactive recommendation function\n",
    "def recommend_kdramas_interactive():\n",
    "    name = input(\"Name of Drama: \").strip()\n",
    "    user_content_rating = input(\"Content Rating (e.g., '15+', '18+ Restricted'): \").strip() or None\n",
    "    year = input(\"Year of Release (leave blank if not specific): \").strip()\n",
    "    year = int(year) if year.isdigit() else None\n",
    "    top_n = 10\n",
    "    \n",
    "    if name not in indices:\n",
    "        return f\"No recommendations found for {name}. This K-Drama might not be in the dataset.\"\n",
    "    \n",
    "    idx = indices[name]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:top_n+1]  # Get top_n recommendations\n",
    "    \n",
    "    kdrama_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    recommended_kdramas = kdramas_df.loc[kdrama_indices].copy()\n",
    "    \n",
    "    if user_content_rating:\n",
    "        recommended_kdramas = recommended_kdramas[recommended_kdramas['Content Rating'].str.contains(user_content_rating)]\n",
    "    \n",
    "    if year:\n",
    "        recommended_kdramas = recommended_kdramas[recommended_kdramas['Year of release'] == year]\n",
    "    \n",
    "    return recommended_kdramas.sort_values(by=['Rating', 'Rank'], ascending=[False, True])['Name'].head(top_n)\n",
    "\n",
    "# Now you can run the interactive function directly\n",
    "recommend_kdramas_interactive()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "10820ada-a468-4823-970a-1d4f79b37791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Name of Drama:  Reply 1988\n",
      "Would you like to filter by specific attributes (yes/no)?  no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2             Hospital Playlist\n",
      "43              Dear My Friends\n",
      "85                   Once Again\n",
      "103             Beautiful World\n",
      "110                  Reply 1997\n",
      "114            Fight For My Way\n",
      "134    The World of the Married\n",
      "161        My Unfamiliar Family\n",
      "175          Seasons of Blossom\n",
      "228                 Light on Me\n",
      "Name: Name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Load the dataset\n",
    "kdramas_df = pd.read_csv('kdrama.csv')\n",
    "\n",
    "# Ensure all relevant columns are string type and fill NaN values\n",
    "columns_to_clean = ['Synopsis', 'Director', 'Screenwriter', 'Cast', 'Genre', 'Tags']\n",
    "for column in columns_to_clean:\n",
    "    kdramas_df[column] = kdramas_df[column].fillna('').astype(str)\n",
    "\n",
    "# Create a combined string of director, screenwriter, and cast\n",
    "kdramas_df['staff'] = kdramas_df['Director'] + ' ' + kdramas_df['Screenwriter'] + ' ' + kdramas_df['Cast']\n",
    "\n",
    "# Creating the 'combined_features' for content-based filtering\n",
    "kdramas_df['combined_features'] = kdramas_df['Synopsis'] + ' ' + kdramas_df['staff'] + ' ' + kdramas_df['Genre'] + ' ' + kdramas_df['Tags']\n",
    "\n",
    "# Initialize TF-IDF Vectorizer and compute the TF-IDF matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(kdramas_df['combined_features'])\n",
    "\n",
    "# Compute the cosine similarity matrix based on the 'combined_features'\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Reverse mapping of indices and K-Drama names\n",
    "indices = pd.Series(kdramas_df.index, index=kdramas_df['Name']).drop_duplicates()\n",
    "\n",
    "# Interactive recommendation function with additional filtering options\n",
    "def recommend_kdramas_interactive():\n",
    "    name = input(\"Name of Drama: \").strip()\n",
    "    use_personalized = input(\"Would you like to filter by specific attributes (yes/no)? \").strip().lower()\n",
    "    \n",
    "    personalized_filters = {}\n",
    "    if use_personalized == 'yes':\n",
    "        personalized_filters['content_rating'] = input(\"Content Rating (e.g., '15+', '18+ Restricted', leave blank if not specific): \").strip() or None\n",
    "        personalized_filters['year'] = input(\"Year of Release (leave blank if not specific): \").strip()\n",
    "        personalized_filters['year'] = int(personalized_filters['year']) if personalized_filters['year'].isdigit() else None\n",
    "        personalized_filters['director'] = input(\"Director's Name (leave blank if not specific): \").strip() or None\n",
    "        personalized_filters['screenwriter'] = input(\"Screenwriter's Name (leave blank if not specific): \").strip() or None\n",
    "        personalized_filters['cast'] = input(\"Cast Member's Name (leave blank if not specific): \").strip() or None\n",
    "        personalized_filters['rating'] = input(\"Minimum Rating (1-10, leave blank if not specific): \").strip()\n",
    "        personalized_filters['rating'] = float(personalized_filters['rating']) if personalized_filters['rating'].replace('.','',1).isdigit() else None\n",
    "        personalized_filters['rank'] = input(\"Maximum Rank (leave blank if not specific): \").strip()\n",
    "        personalized_filters['rank'] = int(personalized_filters['rank']) if personalized_filters['rank'].isdigit() else None\n",
    "        \n",
    "    top_n = 10\n",
    "    \n",
    "    if name not in indices:\n",
    "        return f\"No recommendations found for {name}. This K-Drama might not be in the dataset.\"\n",
    "    \n",
    "    idx = indices[name]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:top_n+1]  # Get top_n recommendations\n",
    "    \n",
    "    kdrama_indices = [i[0] for i in sim_scores]\n",
    "    recommended_kdramas = kdramas_df.loc[kdrama_indices].copy()\n",
    "    \n",
    "    # Apply personalized filters\n",
    "    if personalized_filters.get('content_rating'):\n",
    "        recommended_kdramas = recommended_kdramas[recommended_kdramas['Content Rating'].str.contains(personalized_filters['content_rating'], case=False, na=False)]\n",
    "    if personalized_filters.get('year'):\n",
    "        recommended_kdramas = recommended_kdramas[recommended_kdramas['Year of release'] == personalized_filters['year']]\n",
    "    if personalized_filters.get('director'):\n",
    "        recommended_kdramas = recommended_kdramas[recommended_kdramas['Director'].str.contains(personalized_filters['director'], case=False, na=False)]\n",
    "    if personalized_filters.get('screenwriter'):\n",
    "        recommended_kdramas = recommended_kdramas[recommended_kdramas['Screenwriter'].str.contains(personalized_filters['screenwriter'], case=False, na=False)]\n",
    "    if personalized_filters.get('cast'):\n",
    "                # Filter by cast member's name\n",
    "        recommended_kdramas = recommended_kdramas[recommended_kdramas['Cast'].str.contains(personalized_filters['cast'], case=False, na=False)]\n",
    "    if personalized_filters.get('rating'):\n",
    "        # Filter by minimum rating\n",
    "        recommended_kdramas = recommended_kdramas[recommended_kdramas['Rating'] >= personalized_filters['rating']]\n",
    "    if personalized_filters.get('rank'):\n",
    "        # Filter by maximum rank\n",
    "        recommended_kdramas = recommended_kdramas.sort_values('Rank', ascending=True)\n",
    "        recommended_kdramas = recommended_kdramas[recommended_kdramas['Rank'] <= personalized_filters['rank']]\n",
    "    \n",
    "    # Ensure the dataframe is not empty after applying filters\n",
    "    if recommended_kdramas.empty:\n",
    "        return \"No recommendations found based on the filters provided.\"\n",
    "    \n",
    "    # Return top N recommendations sorted by 'Rating' and 'Rank', if available\n",
    "    recommended_kdramas = recommended_kdramas.sort_values(by=['Rating', 'Rank'], ascending=[False, True])\n",
    "    return recommended_kdramas['Name'].head(top_n)\n",
    "\n",
    "# Run the interactive recommendation function\n",
    "print(recommend_kdramas_interactive())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67343028-028e-43b1-9fba-56b2f37e459d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Flask in c:\\users\\joshu\\anaconda3\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from Flask) (2.2.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from Flask) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from Flask) (2.0.1)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from Flask) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from click>=8.0->Flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from Jinja2>=3.0->Flask) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8e79c88-672b-4e20-82a0-32305e4fed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load your dataset here\n",
    "kdramas_df = pd.read_csv('kdrama.csv')\n",
    "# Assume all other preprocessing steps have been done here...\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def home():\n",
    "    if request.method == 'POST':\n",
    "        # Process the form data and return recommendations\n",
    "        drama_name = request.form.get('drama_name')\n",
    "        # Insert the logic of your recommendation function here\n",
    "        # Use the drama_name to get recommendations\n",
    "        recommendations = recommend_kdramas_interactive(drama_name)  # This needs to be adapted for web usage\n",
    "        return render_template('results.html', recommendations=recommendations)\n",
    "    return render_template('index.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9710324-70b5-4960-a12d-519dc33e6e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
